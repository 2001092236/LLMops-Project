{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG system evaluation\n",
    "\n",
    "In this notebook we will explore how out RAG system works by changing its parameters:\n",
    "\n",
    "1. `top_k_retrieve`\n",
    "2. `top_k_rank`\n",
    "\n",
    "We will measure:\n",
    "1. `responce time`\n",
    "2. `result quality`\n",
    "\n",
    "And will try to accomodate a new approach [LLM-as-a-Judge](https://huggingface.co/learn/cookbook/en/llm_judge) with the [Ragas](https://docs.ragas.io/en/stable/) for evaluation.\n",
    "\n",
    "We will use LLMs from OpenAI and from [Nebius AI Studio API](https://studio.nebius.ai/).\n",
    "\n",
    "Demo usage of Nebius AI is [here](https://github.com/2001092236/LLM-engineering/blob/main/week_1/LLM_engineering_Nebius_AI_Studio_Demo.ipynb).\n",
    "\n",
    "For more detailed desctiption, see my [github](https://github.com/2001092236/LLM-engineering/blob/main/week_2/9.%20Practice%20RAG%20project.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process:\n",
    "\n",
    "1. Create set of $N=10$ questions about HF internals, save them into `.csv`\n",
    "2. Measuring the relevance will be done with the [Ragas](https://docs.ragas.io/en/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sep up functions and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query: str - the text query \n",
    "# chat_id: Optional[str] - chat id to maintain conversation history\n",
    "# model: str = \"gpt-4o-mini\" - name of an LLM\n",
    "# use_reranker: bool = True - whether to rerank after retreival\n",
    "# top_k_retrieve: int = 20 - num of items to retrieve\n",
    "# top_k_rank: int = 4 - num of items to return after reranking\n",
    "# max_out_tokens: int = 512 - max num of tokens. Used to drop retrieved pieces \n",
    "#     of content if total prompt length goes beyond max contex length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from collections import defaultdict\n",
    "\n",
    "class RagChatUser:\n",
    "    \n",
    "    def __init__(self, user_data: dict) -> None:\n",
    "\n",
    "        # user_data = defaultdict(lambda: None)(user_data)\n",
    "        all_fields = (\n",
    "            'api_key',\n",
    "            'user_name',\n",
    "            'password',\n",
    "        )\n",
    "        # Set all fields from user data\n",
    "        \n",
    "        for k in all_fields:\n",
    "            setattr(self, k, None)\n",
    "\n",
    "        for k, v in user_data.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "        if self.api_key is not None:\n",
    "            return\n",
    "                \n",
    "\n",
    "        username, password = user_data['username'], user_data['password']\n",
    "        with httpx.Client() as client:\n",
    "            try:\n",
    "                response = client.post(\n",
    "                    f\"http://0.0.0.0:8001/signup/\",\n",
    "                    data={\n",
    "                        # 'grant_type': 'password',\n",
    "                        \"username\": username,\n",
    "                        \"password\": password,\n",
    "                        # 'scope': '',                # Optional, can be left empty\n",
    "                        # 'client_id': 'string',      # Replace with actual client ID\n",
    "                        # 'client_secret': 'string'   # Replace with actual client secret\n",
    "                    },\n",
    "                    headers={'accept': 'application/json',\n",
    "                                'Content-Type': 'application/x-www-form-urlencoded'},\n",
    "                    timeout=httpx.Timeout(60.0),\n",
    "                )\n",
    "            except httpx.ConnectError:\n",
    "                raise Exception(\"Failed to connect to chat service\")\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(\"Error: filed to create a new user. Try use other username\")\n",
    "            raise Exception(response.json()[\"detail\"])\n",
    "        \n",
    "        print(f\"{response.json()['message']}\")\n",
    "\n",
    "        self.api_key = response.json()['api_key']\n",
    "        print(response.json())\n",
    "\n",
    "\n",
    "        async def ask_rag(self,\n",
    "                  query: str,\n",
    "                  chat_id: str | None = None,\n",
    "                  use_reranker: bool = False, \n",
    "                  top_k_retrieve: int = 20, \n",
    "                  top_k_rank: int = 3, \n",
    "                  model: str = 'gpt-4o-mini',\n",
    "                  ) -> str:\n",
    "            if len(query) == 0:\n",
    "                raise Exception(\"The request is empty, please type something in\")\n",
    "\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                try:\n",
    "                    response = await client.post(\n",
    "                        \"http://0.0.0.0:8001/rag/\",\n",
    "                        json={\"query\": query,\n",
    "                              \"chat_id\": chat_id,\n",
    "                              \"model\": model,\n",
    "                              \"use_reranker\": use_reranker, \n",
    "                              \"top_k_retrieve\": top_k_retrieve, \n",
    "                              \"top_k_rank\": top_k_rank,\n",
    "                              \"max_out_tokens\": 512},\n",
    "                        \n",
    "                        timeout=httpx.Timeout(60.0),\n",
    "                        headers={\"Authorization\": self.api_key,\n",
    "                                 'Content-Type': 'application/json',\n",
    "                                 'accept': 'application/json',\n",
    "                                 },\n",
    "                    )\n",
    "                except httpx.ConnectError:\n",
    "                    raise Exception(\"Failed to connect to RAG service\")\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                raise Exception(response.json()[\"detail\"])\n",
    "            \n",
    "\n",
    "        return resp_message\n",
    "\n",
    "    def ask_rag(self,\n",
    "                  query: str,\n",
    "                  chat_id: str | None = None,\n",
    "                  use_reranker: bool = False, \n",
    "                  top_k_retrieve: int = 20, \n",
    "                  top_k_rank: int = 3, \n",
    "                  model: str = 'gpt-4o-mini',\n",
    "                  ) -> tuple[str, float]:\n",
    "        \n",
    "        if len(query) == 0:\n",
    "            raise Exception(\"The request is empty, please type something in\")\n",
    "\n",
    "        with httpx.Client() as client:\n",
    "            try:\n",
    "                response = client.post(\n",
    "                    \"http://0.0.0.0:8001/rag/\",\n",
    "                    json={\"query\": query,\n",
    "                            \"chat_id\": chat_id,\n",
    "                            \"model\": model,\n",
    "                            \"use_reranker\": use_reranker, \n",
    "                            \"top_k_retrieve\": top_k_retrieve, \n",
    "                            \"top_k_rank\": top_k_rank,\n",
    "                            \"max_out_tokens\": 512},\n",
    "                    \n",
    "                    timeout=httpx.Timeout(60.0),\n",
    "                    headers={\"Authorization\": self.api_key,\n",
    "                                'Content-Type': 'application/json',\n",
    "                                'accept': 'application/json',\n",
    "                                },\n",
    "                )\n",
    "            except httpx.ConnectError:\n",
    "                raise Exception(\"Failed to connect to RAG service\")\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(response.json()[\"detail\"])\n",
    "        \n",
    "\n",
    "        return response.json()['response'], response.elapsed.total_seconds()\n",
    "\n",
    "    async def async_ask_rag(self,\n",
    "                  query: str,\n",
    "                  chat_id: str | None = None,\n",
    "                  use_reranker: bool = False, \n",
    "                  top_k_retrieve: int = 20, \n",
    "                  top_k_rank: int = 3, \n",
    "                  model: str = 'gpt-4o-mini',\n",
    "                  ) -> tuple[str, float]:\n",
    "        \n",
    "        if len(query) == 0:\n",
    "            raise Exception(\"The request is empty, please type something in\")\n",
    "\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            try:\n",
    "                response = await client.post(\n",
    "                    \"http://0.0.0.0:8001/rag/\",\n",
    "                    json={\"query\": query,\n",
    "                            \"chat_id\": chat_id,\n",
    "                            \"model\": model,\n",
    "                            \"use_reranker\": use_reranker, \n",
    "                            \"top_k_retrieve\": top_k_retrieve, \n",
    "                            \"top_k_rank\": top_k_rank,\n",
    "                            \"max_out_tokens\": 512},\n",
    "                    \n",
    "                    timeout=httpx.Timeout(60.0),\n",
    "                    headers={\"Authorization\": self.api_key,\n",
    "                                'Content-Type': 'application/json',\n",
    "                                'accept': 'application/json',\n",
    "                                },\n",
    "                )\n",
    "            except httpx.ConnectError:\n",
    "                raise Exception(\"Failed to connect to RAG service\")\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(response.json()[\"detail\"])\n",
    "        \n",
    "\n",
    "        return response.json()['response'], response.elapsed.total_seconds()\n",
    "    \n",
    "    def ask_llm(self,\n",
    "                        query: str,\n",
    "                        model: str = 'gpt-4o-mini',\n",
    "                        chat_id: str | None = None,\n",
    "                        ) -> str:\n",
    "        \n",
    "        if len(query) == 0:\n",
    "            raise Exception(\"The request is empty, please type something in\")\n",
    "\n",
    "        with httpx.Client() as client:\n",
    "            try:\n",
    "                response = client.post(\n",
    "                    \"http://0.0.0.0:8001/chat/\",\n",
    "                    json={\"message\": query,\n",
    "                            \"chat_id\": chat_id,\n",
    "                            \"model\": model,\n",
    "                            },\n",
    "                    \n",
    "                    timeout=httpx.Timeout(60.0),\n",
    "                    headers={\"Authorization\": self.api_key,\n",
    "                                'Content-Type': 'application/json',\n",
    "                                'accept': 'application/json',\n",
    "                                },\n",
    "                )\n",
    "            except httpx.ConnectError:\n",
    "                raise Exception(\"Failed to connect to RAG service\")\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(response.json()[\"detail\"])\n",
    "        \n",
    "\n",
    "        return response.json()['response']\n",
    "\n",
    "    async def async_ask_llm(self,\n",
    "                        query: str,\n",
    "                        model: str = 'gpt-4o-mini',\n",
    "                        chat_id: str | None = None,\n",
    "                        ) -> str:\n",
    "        \n",
    "        if len(query) == 0:\n",
    "            raise Exception(\"The request is empty, please type something in\")\n",
    "\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            try:\n",
    "                response = await client.post(\n",
    "                    \"http://0.0.0.0:8001/chat/\",\n",
    "                    json={\"message\": query,\n",
    "                            \"chat_id\": chat_id,\n",
    "                            \"model\": model,\n",
    "                            },\n",
    "                    \n",
    "                    timeout=httpx.Timeout(60.0),\n",
    "                    headers={\"Authorization\": self.api_key,\n",
    "                                'Content-Type': 'application/json',\n",
    "                                'accept': 'application/json',\n",
    "                                },\n",
    "                )\n",
    "            except httpx.ConnectError:\n",
    "                raise Exception(\"Failed to connect to RAG service\")\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(response.json()[\"detail\"])\n",
    "        \n",
    "\n",
    "        return response.json()['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI, AsyncOpenAI\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "import json\n",
    "\n",
    "chat_gpt_configs = {\n",
    "    'proxyai': {'base_url': 'https://api.proxyapi.ru/openai/v1', 'api_key': 'sk-3IqehGd2A8iAldf9VgG2CgHyGQtO9qPH'},\n",
    "    'nebius': {'base_url': 'https://api.studio.nebius.ai/v1/', 'api_key': 'eyJhbGciOiJIUzI1NiIsImtpZCI6IlV6SXJWd1h0dnprLVRvdzlLZWstc0M1akptWXBvX1VaVkxUZlpnMDRlOFUiLCJ0eXAiOiJKV1QifQ.eyJzdWIiOiJnb29nbGUtb2F1dGgyfDEwNjQ4NDg2MTk5OTM2MjczNjIzOSIsInNjb3BlIjoib3BlbmlkIG9mZmxpbmVfYWNjZXNzIiwiaXNzIjoiYXBpX2tleV9pc3N1ZXIiLCJhdWQiOlsiaHR0cHM6Ly9uZWJpdXMtaW5mZXJlbmNlLmV1LmF1dGgwLmNvbS9hcGkvdjIvIl0sImV4cCI6MTg4ODIyNjIzMCwidXVpZCI6IjlkNmE3NGU1LWM0MmItNGFlYy05ZGMyLTExZTY3NmRkZDZmMSIsIm5hbWUiOiJBSSZEVCBjb3VyY2UiLCJleHBpcmVzX2F0IjoiMjAyOS0xMS0wMVQxMToxNzoxMCswMDAwIn0.WAYpQB5orr96I9XVRlV2g-OWqDcfTCENCDNSKqE575Y'}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. `top-k` and `top-k-retrieve` analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = RagChatUser({'api_key': 'token-2447608109062291213'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chat.ask_llm(\n",
    "    query=\"Give me a code to train HF model. 5 lines of code\",\n",
    "    model='llama-3-70b',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s a simple example of training a Hugging Face (HF) model using the `Trainer` class:\\n\\n```python\\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\\nfrom transformers import Trainer, TrainingArguments\\nfrom datasets import load_dataset\\n\\n# Load dataset and model\\ndataset = load_dataset(\"glue\", \"sst2\")\\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\\n\\n# Define training arguments and trainer\\ntraining_args = TrainingArguments(\"test_trainer\", num_train_epochs=3, per_device_train_batch_size=16, per_device_eval_batch_size=64, evaluation_strategy=\"epoch\", learning_rate=5.0e-5)\\ntrainer = Trainer(model=model, args=training_args, train_dataset=dataset[\"train\"], eval_dataset=dataset[\"validation\"], compute_metrics=lambda pred: {\"accuracy\": (pred.label_ids == (pred.label_ids > 0.5)).mean()})\\n```\\nThis code doesn\\'t actually train the model - you would need to call the `train` method of the `Trainer` instance to do that:\\n\\n```python\\ntrainer.train()\\n```'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_rag = chat.ask_rag(\n",
    "    query=\"Give me a code to train HF model. 5 lines of code\",\n",
    "    model='llama-3-70b',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Here is a code snippet to train a Hugging Face (HF) model based on the provided context:\\n\\n```python\\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\\n\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"model_checkpoint\")\\ntokenizer = AutoTokenizer.from_pretrained(\"model_checkpoint\")\\n\\ntrain_dataset = tokenizer(train_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\")\\ntrainer = AutoTrainer(\\n    model=model,\\n    train_dataset=train_dataset,\\n    dataset_text_field=\\'text\\',\\n    max_seq_length=512,\\n)\\ntrainer.train()\\n```',\n",
       " 4.279853)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To train a model using Hugging Face's `transformers` library, you can follow these steps. Below is a minimal example assuming you have a dataset ready and you're using a PyTorch model like BERT for a text classification task:\\n\\n```python\\nfrom transformers import BertForSequenceClassification, Trainer, TrainingArguments\\n\\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\\ntraining_args = TrainingArguments(output_dir='./results', num_train_epochs=3, per_device_train_batch_size=8)\\ntrainer = Trainer(model=model, args=training_args, train_dataset=train_dataset)\\n\\ntrainer.train()\\n```\\n\\nThis code assumes that you have a dataset `train_dataset` already loaded and preprocessed. Adjust `num_labels`, `num_train_epochs`, and batch size as needed for your specific task.\""
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. comparison of 3 LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. comparison of 3 LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. different embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. multy-turn conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
